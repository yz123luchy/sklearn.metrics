{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6369c3b2e9b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#首先从sklearn导入metrics模块，并查看有哪些误差评价指标\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCORERS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "#首先从sklearn导入metrics模块，并查看有哪些误差评价指标\n",
    "from sklearn import metrics\n",
    "sorted(sklearn.metrics.SCORERS.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   第一个：accuracy_score（准确率得分）是模型分类正确的数据除以样本总数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>explained_variance</th>\n",
       "      <td>make_scorer(explained_variance_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>make_scorer(r2_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_error</th>\n",
       "      <td>make_scorer(max_error, greater_is_better=False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_median_absolute_error</th>\n",
       "      <td>make_scorer(median_absolute_error, greater_is_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean_absolute_error</th>\n",
       "      <td>make_scorer(mean_absolute_error, greater_is_be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean_squared_error</th>\n",
       "      <td>make_scorer(mean_squared_error, greater_is_bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean_squared_log_error</th>\n",
       "      <td>make_scorer(mean_squared_log_error, greater_is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_root_mean_squared_error</th>\n",
       "      <td>make_scorer(mean_squared_error, greater_is_bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean_poisson_deviance</th>\n",
       "      <td>make_scorer(mean_poisson_deviance, greater_is_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_mean_gamma_deviance</th>\n",
       "      <td>make_scorer(mean_gamma_deviance, greater_is_be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>make_scorer(accuracy_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>make_scorer(roc_auc_score, needs_threshold=True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_ovr</th>\n",
       "      <td>make_scorer(roc_auc_score, needs_proba=True, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_ovo</th>\n",
       "      <td>make_scorer(roc_auc_score, needs_proba=True, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_ovr_weighted</th>\n",
       "      <td>make_scorer(roc_auc_score, needs_proba=True, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_ovo_weighted</th>\n",
       "      <td>make_scorer(roc_auc_score, needs_proba=True, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <td>make_scorer(balanced_accuracy_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_precision</th>\n",
       "      <td>make_scorer(average_precision_score, needs_thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_log_loss</th>\n",
       "      <td>make_scorer(log_loss, greater_is_better=False,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_brier_score</th>\n",
       "      <td>make_scorer(brier_score_loss, greater_is_bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjusted_rand_score</th>\n",
       "      <td>make_scorer(adjusted_rand_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homogeneity_score</th>\n",
       "      <td>make_scorer(homogeneity_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completeness_score</th>\n",
       "      <td>make_scorer(completeness_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_measure_score</th>\n",
       "      <td>make_scorer(v_measure_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mutual_info_score</th>\n",
       "      <td>make_scorer(mutual_info_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjusted_mutual_info_score</th>\n",
       "      <td>make_scorer(adjusted_mutual_info_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalized_mutual_info_score</th>\n",
       "      <td>make_scorer(normalized_mutual_info_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fowlkes_mallows_score</th>\n",
       "      <td>make_scorer(fowlkes_mallows_score)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>make_scorer(precision_score, average=binary)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_macro</th>\n",
       "      <td>make_scorer(precision_score, pos_label=None, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_micro</th>\n",
       "      <td>make_scorer(precision_score, pos_label=None, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_samples</th>\n",
       "      <td>make_scorer(precision_score, pos_label=None, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_weighted</th>\n",
       "      <td>make_scorer(precision_score, pos_label=None, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>make_scorer(recall_score, average=binary)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_macro</th>\n",
       "      <td>make_scorer(recall_score, pos_label=None, aver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_micro</th>\n",
       "      <td>make_scorer(recall_score, pos_label=None, aver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_samples</th>\n",
       "      <td>make_scorer(recall_score, pos_label=None, aver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_weighted</th>\n",
       "      <td>make_scorer(recall_score, pos_label=None, aver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>make_scorer(f1_score, average=binary)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_macro</th>\n",
       "      <td>make_scorer(f1_score, pos_label=None, average=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_micro</th>\n",
       "      <td>make_scorer(f1_score, pos_label=None, average=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_samples</th>\n",
       "      <td>make_scorer(f1_score, pos_label=None, average=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_weighted</th>\n",
       "      <td>make_scorer(f1_score, pos_label=None, average=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard</th>\n",
       "      <td>make_scorer(jaccard_score, average=binary)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_macro</th>\n",
       "      <td>make_scorer(jaccard_score, pos_label=None, ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_micro</th>\n",
       "      <td>make_scorer(jaccard_score, pos_label=None, ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_samples</th>\n",
       "      <td>make_scorer(jaccard_score, pos_label=None, ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard_weighted</th>\n",
       "      <td>make_scorer(jaccard_score, pos_label=None, ave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              0\n",
       "explained_variance                        make_scorer(explained_variance_score)\n",
       "r2                                                        make_scorer(r2_score)\n",
       "max_error                       make_scorer(max_error, greater_is_better=False)\n",
       "neg_median_absolute_error     make_scorer(median_absolute_error, greater_is_...\n",
       "neg_mean_absolute_error       make_scorer(mean_absolute_error, greater_is_be...\n",
       "neg_mean_squared_error        make_scorer(mean_squared_error, greater_is_bet...\n",
       "neg_mean_squared_log_error    make_scorer(mean_squared_log_error, greater_is...\n",
       "neg_root_mean_squared_error   make_scorer(mean_squared_error, greater_is_bet...\n",
       "neg_mean_poisson_deviance     make_scorer(mean_poisson_deviance, greater_is_...\n",
       "neg_mean_gamma_deviance       make_scorer(mean_gamma_deviance, greater_is_be...\n",
       "accuracy                                            make_scorer(accuracy_score)\n",
       "roc_auc                        make_scorer(roc_auc_score, needs_threshold=True)\n",
       "roc_auc_ovr                   make_scorer(roc_auc_score, needs_proba=True, m...\n",
       "roc_auc_ovo                   make_scorer(roc_auc_score, needs_proba=True, m...\n",
       "roc_auc_ovr_weighted          make_scorer(roc_auc_score, needs_proba=True, m...\n",
       "roc_auc_ovo_weighted          make_scorer(roc_auc_score, needs_proba=True, m...\n",
       "balanced_accuracy                          make_scorer(balanced_accuracy_score)\n",
       "average_precision             make_scorer(average_precision_score, needs_thr...\n",
       "neg_log_loss                  make_scorer(log_loss, greater_is_better=False,...\n",
       "neg_brier_score               make_scorer(brier_score_loss, greater_is_bette...\n",
       "adjusted_rand_score                            make_scorer(adjusted_rand_score)\n",
       "homogeneity_score                                make_scorer(homogeneity_score)\n",
       "completeness_score                              make_scorer(completeness_score)\n",
       "v_measure_score                                    make_scorer(v_measure_score)\n",
       "mutual_info_score                                make_scorer(mutual_info_score)\n",
       "adjusted_mutual_info_score              make_scorer(adjusted_mutual_info_score)\n",
       "normalized_mutual_info_score          make_scorer(normalized_mutual_info_score)\n",
       "fowlkes_mallows_score                        make_scorer(fowlkes_mallows_score)\n",
       "precision                          make_scorer(precision_score, average=binary)\n",
       "precision_macro               make_scorer(precision_score, pos_label=None, a...\n",
       "precision_micro               make_scorer(precision_score, pos_label=None, a...\n",
       "precision_samples             make_scorer(precision_score, pos_label=None, a...\n",
       "precision_weighted            make_scorer(precision_score, pos_label=None, a...\n",
       "recall                                make_scorer(recall_score, average=binary)\n",
       "recall_macro                  make_scorer(recall_score, pos_label=None, aver...\n",
       "recall_micro                  make_scorer(recall_score, pos_label=None, aver...\n",
       "recall_samples                make_scorer(recall_score, pos_label=None, aver...\n",
       "recall_weighted               make_scorer(recall_score, pos_label=None, aver...\n",
       "f1                                        make_scorer(f1_score, average=binary)\n",
       "f1_macro                      make_scorer(f1_score, pos_label=None, average=...\n",
       "f1_micro                      make_scorer(f1_score, pos_label=None, average=...\n",
       "f1_samples                    make_scorer(f1_score, pos_label=None, average=...\n",
       "f1_weighted                   make_scorer(f1_score, pos_label=None, average=...\n",
       "jaccard                              make_scorer(jaccard_score, average=binary)\n",
       "jaccard_macro                 make_scorer(jaccard_score, pos_label=None, ave...\n",
       "jaccard_micro                 make_scorer(jaccard_score, pos_label=None, ave...\n",
       "jaccard_samples               make_scorer(jaccard_score, pos_label=None, ave...\n",
       "jaccard_weighted              make_scorer(jaccard_score, pos_label=None, ave..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "pd.DataFrame.from_dict(sklearn.metrics.SCORERS, orient='index', dtype=str, columns=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('error_value.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn \n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.explained_variance的使用\n",
    "#注意查看的是误差指标字典的键，而使用的时候要使用相应的值\n",
    "from sklearn.metrics import explained_variance_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n参数\\nSignature:\\nexplained_variance_score(\\n    y_true,\\n    y_pred,\\n    *,\\n    sample_weight=None,\\n    multioutput='uniform_average',\\n)\\nDocstring:\\nExplained variance regression score function\\n\\nBest possible score is 1.0, lower values are worse.\\n\\nRead more in the :ref:`User Guide <explained_variance_score>`.\\n\\nParameters\\n----------\\n参数格式\\ny_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\\n    Ground truth (correct) target values.\\n\\ny_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\\n    Estimated target values.\\n\\nsample_weight : array-like of shape (n_samples,), optional\\n    Sample weights.\\n\\nmultioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\\n    Defines aggregating of multiple output scores.\\n    Array-like value defines weights used to average scores.\\n\\n    'raw_values' :\\n        Returns a full set of scores in case of multioutput input.\\n\\n    'uniform_average' :\\n        Scores of all outputs are averaged with uniform weight.\\n\\n    'variance_weighted' :\\n        Scores of all outputs are averaged, weighted by the variances\\n        of each individual output.\\n\\nReturns\\n-------\\nscore : float or ndarray of floats\\n    The explained variance or ndarray if 'multioutput' is 'raw_values'.\\n\\nNotes\\n-----\\nThis is not a symmetric function.\\n    \\n    \""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "参数\n",
    "Signature:\n",
    "explained_variance_score(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput='uniform_average',\n",
    ")\n",
    "Docstring:\n",
    "Explained variance regression score function\n",
    "\n",
    "Best possible score is 1.0, lower values are worse.\n",
    "\n",
    "Read more in the :ref:`User Guide <explained_variance_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "参数格式\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), optional\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : string in ['raw_values', 'uniform_average',                 'variance_weighted'] or array-like of shape (n_outputs)\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "score : float or ndarray of floats\n",
    "    The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9571734475374732\n",
      "0.9571734475374732\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(explained_variance_score(y_true, y_pred))\n",
    "\n",
    "y_true = [[3], [-0.5], [2], [7]]\n",
    "y_pred = [[2.5], [0], [2], [8]]\n",
    "print(explained_variance_score(y_true, y_pred, multioutput='uniform_average'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multioutput：多维输入输出，可选‘raw_values’, ‘uniform_average’, ‘variance_weighted’或None。默认为’uniform_average’;\n",
    "#raw_values：分别返回各维度得分\n",
    "#uniform_average：各输出维度得分的平均\n",
    "#variance_weighted：对所有输出的分数进行平均，并根据每个输出的方差进行加权。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.explained_variance的使用\n",
    "#注意查看的是误差指标字典的键，而使用的时候要使用相应的值\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Signature:\n",
    "r2_score(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    *,\n",
    "    sample_weight=None,\n",
    "    multioutput='uniform_average',\n",
    ")\n",
    "Docstring:\n",
    "R^2 (coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the\n",
    "model can be arbitrarily worse). A constant model that always\n",
    "predicts the expected value of y, disregarding the input features,\n",
    "would get a R^2 score of 0.0.\n",
    "\n",
    "Read more in the :ref:`User Guide <r2_score>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "sample_weight : array-like of shape (n_samples,), optional\n",
    "    Sample weights.\n",
    "\n",
    "multioutput : string in ['raw_values', 'uniform_average', 'variance_weighted'] or None or array-like of shape (n_outputs)\n",
    "\n",
    "    Defines aggregating of multiple output scores.\n",
    "    Array-like value defines weights used to average scores.\n",
    "    Default is \"uniform_average\".\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of scores in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "    'variance_weighted' :\n",
    "        Scores of all outputs are averaged, weighted by the variances\n",
    "        of each individual output.\n",
    "\n",
    "    .. versionchanged:: 0.19\n",
    "        Default value of multioutput is 'uniform_average'.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "z : float or ndarray of floats\n",
    "    The R^2 score or ndarray of scores if 'multioutput' is\n",
    "    'raw_values'.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This is not a symmetric function.\n",
    "\n",
    "Unlike most other scores, R^2 score may be negative (it need not actually\n",
    "be the square of a quantity R).\n",
    "\n",
    "This metric is not well-defined for single samples and will return a NaN\n",
    "value if n_samples is less than two.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [1] `Wikipedia entry on the Coefficient of determination\n",
    "        <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486081370449679\n",
      "0.9382566585956417\n",
      "1.0\n",
      "0.0\n",
      "-3.0\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(r2_score(y_true, y_pred,multioutput='variance_weighted'))\n",
    "\n",
    "\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [1, 2, 3]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [2, 2, 2]\n",
    "print(r2_score(y_true, y_pred))\n",
    "\n",
    "y_true = [1, 2, 3]\n",
    "y_pred = [3, 2, 1]\n",
    "print(r2_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import max_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSignature: max_error(y_true, y_pred)\\nDocstring:\\nmax_error metric calculates the maximum residual error.\\n\\nRead more in the :ref:`User Guide <max_error>`.\\n\\nParameters\\n----------\\ny_true : array-like of shape (n_samples,)\\n    Ground truth (correct) target values.\\n\\ny_pred : array-like of shape (n_samples,)\\n    Estimated target values.\\n\\nReturns\\n-------\\nmax_error : float\\n    A positive floating point value (the best value is 0.0).\\n\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Signature: max_error(y_true, y_pred)\n",
    "Docstring:\n",
    "max_error metric calculates the maximum residual error.\n",
    "\n",
    "Read more in the :ref:`User Guide <max_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape (n_samples,)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape (n_samples,)\n",
    "    Estimated target values.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "max_error : float\n",
    "    A positive floating point value (the best value is 0.0).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "from sklearn.metrics import max_error\n",
    "y_true = [3, 2, 7, 1]\n",
    "y_pred = [10, 5, 4, 9]\n",
    "print(max_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSignature: median_absolute_error(y_true, y_pred, *, multioutput='uniform_average')\\nDocstring:\\nMedian absolute error regression loss\\n\\nMedian absolute error output is non-negative floating point. The best value\\nis 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\\n\\nParameters\\n----------\\ny_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\\n    Ground truth (correct) target values.\\n\\ny_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\\n    Estimated target values.\\n\\nmultioutput : {'raw_values', 'uniform_average'} or array-like of shape                 (n_outputs,)\\n    Defines aggregating of multiple output values. Array-like value defines\\n    weights used to average errors.\\n\\n    'raw_values' :\\n        Returns a full set of errors in case of multioutput input.\\n\\n    'uniform_average' :\\n        Errors of all outputs are averaged with uniform weight.\\n\\nReturns\\n-------\\nloss : float or ndarray of floats\\n    If multioutput is 'raw_values', then mean absolute error is returned\\n    for each output separately.\\n    If multioutput is 'uniform_average' or an ndarray of weights, then the\\n    weighted average of all output errors is returned.\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Signature: median_absolute_error(y_true, y_pred, *, multioutput='uniform_average')\n",
    "Docstring:\n",
    "Median absolute error regression loss\n",
    "\n",
    "Median absolute error output is non-negative floating point. The best value\n",
    "is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
    "    Estimated target values.\n",
    "\n",
    "multioutput : {'raw_values', 'uniform_average'} or array-like of shape                 (n_outputs,)\n",
    "    Defines aggregating of multiple output values. Array-like value defines\n",
    "    weights used to average errors.\n",
    "\n",
    "    'raw_values' :\n",
    "        Returns a full set of errors in case of multioutput input.\n",
    "\n",
    "    'uniform_average' :\n",
    "        Errors of all outputs are averaged with uniform weight.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "loss : float or ndarray of floats\n",
    "    If multioutput is 'raw_values', then mean absolute error is returned\n",
    "    for each output separately.\n",
    "    If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
    "    weighted average of all output errors is returned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.75\n",
      "[0.5 1. ]\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "from sklearn.metrics import median_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "print(median_absolute_error(y_true, y_pred))\n",
    "\n",
    "y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
    "print(median_absolute_error(y_true, y_pred))\n",
    "\n",
    "print(median_absolute_error(y_true, y_pred, multioutput='raw_values'))\n",
    "\n",
    "print(median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
